{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12: Unit tests for analytics\n",
    "\n",
    "When we write code, we want to make sure that it works as expected. One way to do this is using tests. There are different types of tests:\n",
    "\n",
    "- **Unit tests**: These tests are used to test individual units of code. They are usually small and test a single function or method.\n",
    "- **Integration tests**: These tests are used to test how different parts of the code work together.\n",
    "- **End-to-end tests**: These tests are used to test the entire system.\n",
    "\n",
    "In this session we will focus on unit tests. We will write tests for the analytics functions that we have implemented in the past.\n",
    "\n",
    "We will use the `pytest` library to write and run the tests. `pytest` is a popular testing framework for Python. It makes it easy to write simple and scalable tests.\n",
    "\n",
    "But before anything, we need to understand how to deal with things when they go wrong in Python: errors and exceptions.\n",
    "\n",
    "## Catching errors and exceptions\n",
    "\n",
    "Whenever we do something wrong in Python, an error or an exception is raised. For example, if we try to divide by zero, a `ZeroDivisionError` is raised. If we try to access an element in a list that does not exist, an `IndexError` is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m my_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m my_list[\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "my_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "my_list[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IndexError` is raised when you try to access an index that is out of range.\n",
    "\n",
    "In this case, the index 5 is out of range for the list `my_list` because the list only has indices 0 to 4. To fix this error, you can either access an index within the range of the list or handle the error using a try-except block. Here is an example of how you can handle the error using a try-except block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An IndexError occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "try: # try to execute the code\n",
    "    my_list[5]\n",
    "except IndexError as e: # catching the exception and storing it in a variable\n",
    "    print(f\"An IndexError occurred: {e}\")\n",
    "else: # if no exception occurred\n",
    "    print(\"No exception occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between an error and an exception is that an error is a problem that occurs at runtime and stops the execution of the program, while an exception is a problem that occurs at runtime but can be handled by the program.\n",
    "\n",
    "In summary, when an error occurs, the program stops executing, while when an exception occurs, the program can continue executing if the exception is handled.\n",
    "\n",
    "So the ideal is that we know what can go wrong in our code and handle these situations. This is where unit tests come in. They help us to identify these situations and handle them.\n",
    "\n",
    "## General syntax for catching exceptions\n",
    "\n",
    "The general syntax for catching exceptions in Python is as follows:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # code that may raise an exception\n",
    "except ExceptionType as e:\n",
    "    # code to handle the exception\n",
    "```\n",
    "\n",
    "We can catch several exceptions at once by using a tuple of exception types:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # code that may raise an exception\n",
    "except (ExceptionType1, ExceptionType2) as e:\n",
    "    # code to handle the exception\n",
    "```\n",
    "\n",
    "We can also catch all exceptions by using the base class `Exception`:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # code that may raise an exception\n",
    "except Exception as e:\n",
    "    # code to handle the exception\n",
    "```\n",
    "\n",
    "By using `Exception`, we can catch all exceptions, but it is not recommended to do so because it can hide bugs in the code. It is better to catch specific exceptions that you know how to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raising exceptions\n",
    "\n",
    "You can also raise exceptions in Python using the `raise` statement. This is useful when you want to stop the execution of the program and raise an exception.\n",
    "\n",
    "Here is an example of how you can raise an exception in Python:\n",
    "\n",
    "```python\n",
    "raise Exception(\"An error occurred\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Cannot divide by zero, dummy.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denom \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot divide by zero, dummy.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     num \u001b[38;5;241m/\u001b[39m denom\n",
      "\u001b[1;31mException\u001b[0m: Cannot divide by zero, dummy."
     ]
    }
   ],
   "source": [
    "num = 5\n",
    "denom = 0\n",
    "\n",
    "if denom == 0:\n",
    "    raise Exception(\"Cannot divide by zero, dummy.\")\n",
    "else:\n",
    "    num / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot divide by zero, dummy.\n"
     ]
    }
   ],
   "source": [
    "num = 5\n",
    "denom = 0\n",
    "try: \n",
    "    if denom == 0:\n",
    "        raise Exception(\"Cannot divide by zero, dummy.\")\n",
    "    else:\n",
    "        print(num / denom)\n",
    "\n",
    "except Exception as e:\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have seen how to catch exceptions and how to raise exceptions in Python. Now let's see how we can write unit tests for our analytics functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `assert` statement\n",
    "\n",
    "Before we start writing tests, let's talk about the `assert` statement. The `assert` statement is used to check if an expression is `True`. If the expression is `False`, an `AssertionError` exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1+1 == 2, \"1+1 should be 2\"  # all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "1+1 should be 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1+1 should be 2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 1+1 should be 2"
     ]
    }
   ],
   "source": [
    "assert 1+1 == 3, \"1+1 should be 2\"  # this will raise an AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assert statement has the following syntax:\n",
    "\n",
    "```python\n",
    "assert expression, message\n",
    "```\n",
    "\n",
    "If the expression is `False`, the message is printed along with the `AssertionError` exception.\n",
    "\n",
    "We are going to use the `assert` statement to write our tests in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dum",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num \u001b[38;5;241m/\u001b[39m denom \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDum\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Dum"
     ]
    }
   ],
   "source": [
    "assert num / denom == 2.5, 'Dum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing tests\n",
    "\n",
    "To write tests, we need to create a new file with the name `test_<module>.py`. For example, if we have a module called `analytics.py`, we need to create a file called `test_analytics.py`.\n",
    "\n",
    "In this file, we will write functions that start with `test_`. These functions will contain the tests for the functions that we want to test.\n",
    "\n",
    "For example, if we have a function called `add` in the `analytics.py` module, we can write a test for it like this:\n",
    "\n",
    "```python\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3\n",
    "    assert add(0, 0) == 0\n",
    "    assert add(-1, 1) == 0\n",
    "```\n",
    "\n",
    "Within the test function, we use the `assert` statement to check if the function `add` works as expected. It´s very easy to make a test pass if you use silly assertions, so make sure to test the function with different inputs and think of corner cases to include in oyur tests.\n",
    "\n",
    "Also try to test things that should not happen, like passing a string to a function that expects an integer.\n",
    "\n",
    "## Running tests\n",
    "\n",
    "As mentioned, we will store our functions in a file called `analytics.py` and our tests in a file called `test_analytics.py`.\n",
    "\n",
    "To run the tests, we need to execute the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "pytest test_analytics.py\n",
    "```\n",
    "\n",
    "This command will run all the tests in the `test_analytics.py` file.\n",
    "\n",
    "## Coverage\n",
    "\n",
    "When we write tests, we want to make sure that we are testing all the code that we have written. We can use the `coverage` library to check the code coverage of our tests.\n",
    "\n",
    "The concept of coverage is simple: it tells us how many lines of code are executed when we run the tests. The goal is to have 100% coverage, which means that all the lines of code are executed at least once.\n",
    "\n",
    "With this information, we can identify which parts of the code are not being tested and write more tests to cover them.\n",
    "\n",
    "To check the coverage of our tests, we need to install the `coverage` library:\n",
    "\n",
    "```bash\n",
    "pip install coverage\n",
    "```\n",
    "\n",
    "Then we can run the following command to check the coverage:\n",
    "\n",
    "```bash\n",
    "coverage run -m pytest test_analytics_1.py\n",
    "```\n",
    "\n",
    "This command will run the tests and check the coverage. To see the coverage report, we can run the following command:\n",
    "\n",
    "```bash\n",
    "coverage report\n",
    "```\n",
    "\n",
    "With this, we are ready to start writing tests for our analytics functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Build a function called `stats_report` in the `analytics_1.py` script that receives a list of numbers and returns a dictionary with the following statistics:\n",
    "\n",
    "- `mean`: The average of the numbers.\n",
    "- `median`: The median of the numbers.\n",
    "- `std`: The standard deviation of the numbers.\n",
    "\n",
    "Then, write tests for this function in the `test_analytics_1.py` script. The `test_analytics_1.py` script should call the `stats_report` from `analytics_1.py` function with different inputs and check if the output is correct.\n",
    "\n",
    "After building the tests, run them using the `pytest` command and check the coverage using the `coverage` library.\n",
    "\n",
    "```bash\n",
    "pytest test_analytics_1.py\n",
    "coverage run -m pytest test_analytics_1.py\n",
    "coverage report\n",
    "coverage html\n",
    "```\n",
    "\n",
    "After running pytest, you should see the following output:\n",
    "\n",
    "```bash\n",
    "======================================================= test session starts =======================================================\n",
    "platform darwin -- Python 3.10.13, pytest-8.3.4, pluggy-1.5.0\n",
    "rootdir: /Users/dgh/Desktop/PDA/pda2/class_material\n",
    "plugins: Faker-24.4.0, typeguard-4.3.0, anyio-4.2.0\n",
    "collected 4 items                                                                                                                 \n",
    "\n",
    "test_analytics_1.py ....                                                                                                    [100%]\n",
    "\n",
    "======================================================== 4 passed in 0.13s ========================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that 4 tests passed. Since we have 4 tests, we have 100% of tests passing.\n",
    "\n",
    "But what about the coverage? Let´s check it:\n",
    "\n",
    "```bash\n",
    "Name                  Stmts   Miss  Cover\n",
    "-----------------------------------------\n",
    "analytics_1.py           10      2    80%\n",
    "test_analytics_1.py      21      1    95%\n",
    "-----------------------------------------\n",
    "TOTAL                    31      3    90%\n",
    "```\n",
    "\n",
    "We have 90% of coverage. That means that we are not testing all the lines of code in our scripts. We should write more tests to cover the missing lines.\n",
    "\n",
    "With the `coverage html` command, we can generate a report in HTML format. This report will show us which lines of code are not being tested. Open the `htmlcov/index.html` file in your browser to see the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests that cover for errors\n",
    "\n",
    "When writing tests, we should also test for errors. We can use the `pytest.raises` function to check if a function raises an exception.\n",
    "\n",
    "For example, if we have a function called `divide` that divides two numbers, we can write a test like this:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def test_divide_by_zero():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        divide(1, 0)\n",
    "```\n",
    "\n",
    "In this test, we use the `pytest.raises` function to check if the function `divide` raises a `ZeroDivisionError` exception when we try to divide by zero.\n",
    "\n",
    "If the function raises the exception, the test passes. If the function does not raise the exception, the test fails. So we have to include the error raising in our function to make the test pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Add a new function called `divide` to the `analytics_1.py` script. This function should receive two numbers and return the result of dividing the first number by the second number.\n",
    "\n",
    "Then, write tests for this function in the `test_analytics_1.py` script. The `test_analytics_1.py` script should call the `divide` from `analytics_1.py` function with different inputs and check if the output is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Create a script that calculates the factorial of a number. The factorial of a number `n` is the product of all positive integers less than or equal to `n`. For example, the factorial of `5` is `5 * 4 * 3 * 2 * 1 = 120`.\n",
    "\n",
    "Then, create another script with tests for this function. The tests should check if the function calculates the factorial of a number correctly.\n",
    "\n",
    "Include the following tests:\n",
    "\n",
    "- Test the factorial of `0`, should return `1`.\n",
    "- Test the factorial of `1`, should return `1`.\n",
    "- Test the factorial of `5`, should return `120`.\n",
    "- Test the factorial of `'a'`, should raise a `TypeError` exception because the input is not an integer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
